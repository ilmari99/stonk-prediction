{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 12:06:01.556424: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-18 12:06:01.580510: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-18 12:06:01.580539: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-18 12:06:01.580544: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 12:06:01.584567: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.3 when it was built against 1.14.2, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import locale\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "keras.saving.get_custom_objects().clear()\n",
    "\n",
    "from stock_modules.stock_transform import (create_batch_xy,\n",
    "                                           create_transformer_onehot_xy)\n",
    "from stock_modules.stock_embed import timestamps_to_marks\n",
    "from stock_modules.stock_autoformer import Autoformer\n",
    "\n",
    "from invest_strategies import (calculate_optimal_invest_strategy,\n",
    "                               calculate_profit_on_invest_strategy,\n",
    "                               strategy_mask_from_direction_model)\n",
    "from stock_modules.stock_plot import plot_mask_and_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODING = locale.getpreferredencoding()\n",
    "DF_PATH = \"HEL_12-10-21to08-11-23.csv\"\n",
    "MODEL_SERIAL = None\n",
    "SELECTED_TICKERS_PATH = \"./TICKERS_TO_FOLLOW.json\"\n",
    "\n",
    "TEST_FRAC = 0.2\n",
    "PREDICT_PRICES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected tickers: \n",
      " ['ALBBV.HE', 'CGCBV.HE', 'EQV1V.HE', 'KNEBV.HE', 'ORNBV.HE', 'OLVAS.HE', 'DETEC.HE', 'PON1V.HE', 'ORNAV.HE', 'VALMT.HE', 'NESTE.HE', 'HUH1V.HE', 'REG1V.HE', 'VAIAS.HE']\n",
      "Dataframe columns: \n",
      " Index(['REG1V.HE', 'NESTE.HE', 'ORNBV.HE', 'KNEBV.HE', 'OLVAS.HE', 'HUH1V.HE',\n",
      "       'DETEC.HE', 'ORNAV.HE', 'CGCBV.HE', 'VAIAS.HE', 'ALBBV.HE', 'VALMT.HE',\n",
      "       'EQV1V.HE', 'PON1V.HE'],\n",
      "      dtype='object')\n",
      "Dataframe shape:  (4389, 14)\n",
      "Dataframe head: \n",
      "                       REG1V.HE   NESTE.HE   ORNBV.HE   KNEBV.HE   OLVAS.HE  \\\n",
      "date                                                                         \n",
      "2021-10-12 07:00:00  55.950001  41.820000  35.689999  60.220001  53.099998   \n",
      "2021-10-12 08:00:00  55.799999  41.720001  35.630001  60.419998  53.299999   \n",
      "\n",
      "                      HUH1V.HE  DETEC.HE   ORNAV.HE   CGCBV.HE   VAIAS.HE  \\\n",
      "date                                                                        \n",
      "2021-10-12 07:00:00  38.529999      23.0  38.049999  43.139999  46.150002   \n",
      "2021-10-12 08:00:00  38.560001      23.0  38.049999  43.500000  45.950001   \n",
      "\n",
      "                      ALBBV.HE   VALMT.HE   EQV1V.HE   PON1V.HE  \n",
      "date                                                             \n",
      "2021-10-12 07:00:00  28.700001  36.459999  24.850000  39.150002  \n",
      "2021-10-12 08:00:00  28.799999  36.599998  24.950001  39.200001  \n",
      "Index conversion: \n",
      " {0: 'REG1V.HE', 1: 'NESTE.HE', 2: 'ORNBV.HE', 3: 'KNEBV.HE', 4: 'OLVAS.HE', 5: 'HUH1V.HE', 6: 'DETEC.HE', 7: 'ORNAV.HE', 8: 'CGCBV.HE', 9: 'VAIAS.HE', 10: 'ALBBV.HE', 11: 'VALMT.HE', 12: 'EQV1V.HE', 13: 'PON1V.HE'}\n"
     ]
    }
   ],
   "source": [
    "SELECTED_TICKERS = json.load(open(SELECTED_TICKERS_PATH,\n",
    "                                  \"r\", encoding=ENCODING))\n",
    "DATAFRAME = pd.read_csv(DF_PATH, encoding=ENCODING)\n",
    "\n",
    "DATAFRAME.set_index(\"date\", inplace=True)\n",
    "HAS_TIMEDELTA = \"Time Delta\" in DATAFRAME.columns\n",
    "\n",
    "# ind transformation tells the label of each index in the np_arr_test\n",
    "IND_CONVERSION = {i: ticker for i, ticker in enumerate(DATAFRAME.columns) if ticker in SELECTED_TICKERS}\n",
    "IND_CONVERSION = {i: ticker for i, ticker in enumerate(IND_CONVERSION.values())}\n",
    "\n",
    "print(\"Selected tickers: \\n\", SELECTED_TICKERS)\n",
    "print(\"Dataframe columns: \\n\", DATAFRAME.columns)\n",
    "print(\"Dataframe shape: \", DATAFRAME.shape)\n",
    "print(\"Dataframe head: \\n\", DATAFRAME.head(2))\n",
    "print(f\"Index conversion: \\n {IND_CONVERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed df: \n",
      "                      REG1V.HE  NESTE.HE  ORNBV.HE  KNEBV.HE  OLVAS.HE  \\\n",
      "date                                                                    \n",
      "2021-10-12 07:00:00 -0.150002 -0.099998 -0.059998  0.199997  0.200001   \n",
      "2021-10-12 08:00:00 -0.150002 -0.099998 -0.059998  0.199997  0.200001   \n",
      "\n",
      "                     HUH1V.HE  DETEC.HE  ORNAV.HE  CGCBV.HE  VAIAS.HE  \\\n",
      "date                                                                    \n",
      "2021-10-12 07:00:00  0.030003       0.0       0.0  0.360001 -0.200001   \n",
      "2021-10-12 08:00:00  0.030003       0.0       0.0  0.360001 -0.200001   \n",
      "\n",
      "                     ALBBV.HE  VALMT.HE  EQV1V.HE  PON1V.HE  \n",
      "date                                                         \n",
      "2021-10-12 07:00:00  0.099998  0.139999       0.1  0.049999  \n",
      "2021-10-12 08:00:00  0.099998  0.139999       0.1  0.049999  \n",
      "Transformed df shape:  (4389, 14)\n"
     ]
    }
   ],
   "source": [
    "test_begin_idx = int(DATAFRAME.shape[0] * (1 - TEST_FRAC))\n",
    "\n",
    "if PREDICT_PRICES:\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    scaler.fit(DATAFRAME.iloc[:test_begin_idx, :])\n",
    "    transformed_df = pd.DataFrame(scaler.transform(DATAFRAME), columns=DATAFRAME.columns, index=DATAFRAME.index)\n",
    "    transformed_np_arr = transformed_df.to_numpy()\n",
    "\n",
    "    def inverse_transform(df):\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            return pd.DataFrame(scaler.inverse_transform(df), columns=df.columns, index=df.index)\n",
    "        elif isinstance(df, np.ndarray):\n",
    "            return scaler.inverse_transform(df)\n",
    "\n",
    "# If we are predicting the up/down, we create a dataframe where we subtract the previous value from the current value\n",
    "else:\n",
    "    # Do not diff the Time Delta column\n",
    "    df = DATAFRAME.copy()\n",
    "    if HAS_TIMEDELTA:\n",
    "        td_col = df[\"Time Delta\"]\n",
    "        df.drop(\"Time Delta\", axis=1, inplace=True)\n",
    "    transformed_df = df.diff()\n",
    "    # The first row is NaN, so lets copy the second row there\n",
    "    transformed_df.iloc[0, :] = transformed_df.iloc[1, :]\n",
    "    # Add back the Time Delta column\n",
    "    if HAS_TIMEDELTA:\n",
    "        transformed_df[\"Time Delta\"] = td_col\n",
    "        # Make Time Delta the first column\n",
    "        cols = transformed_df.columns.tolist()\n",
    "        cols = cols[-1:] + cols[:-1]\n",
    "        transformed_df = transformed_df[cols]\n",
    "    transformed_np_arr = transformed_df.to_numpy()\n",
    "\n",
    "    def inverse_transform(df):\n",
    "        return df\n",
    "\n",
    "print(\"Transformed df: \\n\", transformed_df.head(2))\n",
    "print(\"Transformed df shape: \", transformed_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 12:06:05.954307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-18 12:06:05.971590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-18 12:06:05.971746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-18 12:06:05.972680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-18 12:06:05.972793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-18 12:06:05.972853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-18 12:06:06.004416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-18 12:06:06.004537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-18 12:06:06.004611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-18 12:06:06.004665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2288 MB memory:  -> device: 0, name: NVIDIA RTX A500 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "[None, 52, 3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filecc3l4_i3.py\", line 15, in tf__call\n        y_dec_s, y_dec_t = ag__.converted_call(ag__.ld(self).decoder, ([ag__.ld(x_dec_s), ag__.ld(y_enc), ag__.ld(x_dec_t)],), None, fscope)\n    File \"/tmp/__autograph_generated_filecgzz3nml.py\", line 28, in tf__call\n        ag__.for_stmt(ag__.ld(self).dec_layers, None, loop_body, get_state, set_state, ('x', 'xt'), {'iterate_names': 'dec_layer'})\n    File \"/tmp/__autograph_generated_filecgzz3nml.py\", line 25, in loop_body\n        xt += residual_trend\n\n    ValueError: Exception encountered when calling layer 'autoformer' (type Autoformer).\n    \n    in user code:\n    \n        File \"/home/sergiovaneg/stonk-prediction/stock_modules/stock_autoformer.py\", line 466, in call  *\n            y_dec_s, y_dec_t = self.decoder([x_dec_s,y_enc,x_dec_t])\n        File \"/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_filecgzz3nml.py\", line 28, in tf__call\n            ag__.for_stmt(ag__.ld(self).dec_layers, None, loop_body, get_state, set_state, ('x', 'xt'), {'iterate_names': 'dec_layer'})\n        File \"/tmp/__autograph_generated_filecgzz3nml.py\", line 25, in loop_body\n            xt += residual_trend\n    \n        ValueError: Exception encountered when calling layer 'corr_decoder' (type CorrDecoder).\n        \n        in user code:\n        \n            File \"/home/sergiovaneg/stonk-prediction/stock_modules/stock_autoformer.py\", line 376, in call  *\n                xt += residual_trend\n        \n            ValueError: Dimensions must be equal, but are 14 and 3 for '{{node autoformer/corr_decoder/add}} = AddV2[T=DT_FLOAT](autoformer/concat, autoformer/corr_decoder/corr_decoder_layer/conv1d_1/Conv1D/Squeeze)' with input shapes: [?,52,14], [?,52,3].\n        \n        \n        Call arguments received by layer 'corr_decoder' (type CorrDecoder):\n          • inputs=['tf.Tensor(shape=(None, 52, 32), dtype=float32)', 'tf.Tensor(shape=(None, 101, 32), dtype=float32)', 'tf.Tensor(shape=(None, 52, 14), dtype=float32)']\n    \n    \n    Call arguments received by layer 'autoformer' (type Autoformer):\n      • inputs=('tf.Tensor(shape=(None, 101, 14), dtype=float32)', 'tf.Tensor(shape=(None, 101, 5), dtype=int32)', 'tf.Tensor(shape=(None, 1, 5), dtype=int32)')\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 79\u001b[0m\n\u001b[1;32m     73\u001b[0m model \u001b[38;5;241m=\u001b[39m Autoformer(model_dict)\n\u001b[1;32m     75\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[1;32m     76\u001b[0m             loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(),\n\u001b[1;32m     77\u001b[0m             metrics\u001b[38;5;241m=\u001b[39m[keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAccuracy()])\n\u001b[0;32m---> 79\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_ts_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_ts_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     89\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mserial\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m model_dict\u001b[38;5;241m.\u001b[39mupdate(model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m     95\u001b[0m             x \u001b[38;5;241m=\u001b[39m(x_test,\n\u001b[1;32m     96\u001b[0m                 x_ts_test,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m         )\n\u001b[1;32m    104\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filev7lr0696.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filecc3l4_i3.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     13\u001b[0m x_dec_s \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39membed, ([ag__\u001b[38;5;241m.\u001b[39mld(x_dec_s), ag__\u001b[38;5;241m.\u001b[39mld(x_dec_marks)],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     14\u001b[0m y_enc \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mencoder, (ag__\u001b[38;5;241m.\u001b[39mld(x_enc),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 15\u001b[0m y_dec_s, y_dec_t \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dec_s\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_enc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dec_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m y_dec \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_dec_s) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_dec_t)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filecgzz3nml.py:28\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     26\u001b[0m dec_layer \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdec_layer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m residual_trend \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresidual_trend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdec_layer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mnorm_layer, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mout_proj, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filecgzz3nml.py:25\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     23\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(residual_trend)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mas_list, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope))\n\u001b[1;32m     24\u001b[0m xt \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(xt)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mxt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresidual_trend\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filecc3l4_i3.py\", line 15, in tf__call\n        y_dec_s, y_dec_t = ag__.converted_call(ag__.ld(self).decoder, ([ag__.ld(x_dec_s), ag__.ld(y_enc), ag__.ld(x_dec_t)],), None, fscope)\n    File \"/tmp/__autograph_generated_filecgzz3nml.py\", line 28, in tf__call\n        ag__.for_stmt(ag__.ld(self).dec_layers, None, loop_body, get_state, set_state, ('x', 'xt'), {'iterate_names': 'dec_layer'})\n    File \"/tmp/__autograph_generated_filecgzz3nml.py\", line 25, in loop_body\n        xt += residual_trend\n\n    ValueError: Exception encountered when calling layer 'autoformer' (type Autoformer).\n    \n    in user code:\n    \n        File \"/home/sergiovaneg/stonk-prediction/stock_modules/stock_autoformer.py\", line 466, in call  *\n            y_dec_s, y_dec_t = self.decoder([x_dec_s,y_enc,x_dec_t])\n        File \"/home/sergiovaneg/miniforge3/envs/stonks/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_filecgzz3nml.py\", line 28, in tf__call\n            ag__.for_stmt(ag__.ld(self).dec_layers, None, loop_body, get_state, set_state, ('x', 'xt'), {'iterate_names': 'dec_layer'})\n        File \"/tmp/__autograph_generated_filecgzz3nml.py\", line 25, in loop_body\n            xt += residual_trend\n    \n        ValueError: Exception encountered when calling layer 'corr_decoder' (type CorrDecoder).\n        \n        in user code:\n        \n            File \"/home/sergiovaneg/stonk-prediction/stock_modules/stock_autoformer.py\", line 376, in call  *\n                xt += residual_trend\n        \n            ValueError: Dimensions must be equal, but are 14 and 3 for '{{node autoformer/corr_decoder/add}} = AddV2[T=DT_FLOAT](autoformer/concat, autoformer/corr_decoder/corr_decoder_layer/conv1d_1/Conv1D/Squeeze)' with input shapes: [?,52,14], [?,52,3].\n        \n        \n        Call arguments received by layer 'corr_decoder' (type CorrDecoder):\n          • inputs=['tf.Tensor(shape=(None, 52, 32), dtype=float32)', 'tf.Tensor(shape=(None, 101, 32), dtype=float32)', 'tf.Tensor(shape=(None, 52, 14), dtype=float32)']\n    \n    \n    Call arguments received by layer 'autoformer' (type Autoformer):\n      • inputs=('tf.Tensor(shape=(None, 101, 14), dtype=float32)', 'tf.Tensor(shape=(None, 101, 5), dtype=int32)', 'tf.Tensor(shape=(None, 1, 5), dtype=int32)')\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./models/\"):\n",
    "    os.makedirs(\"./models/\")\n",
    "\n",
    "CLASS_FIRST = True\n",
    "\n",
    "if MODEL_SERIAL is None:\n",
    "    counter = 0\n",
    "    RESUME = 0\n",
    "\n",
    "    for memory_length in [100]:\n",
    "        if PREDICT_PRICES:\n",
    "            OUTPUT_SCALE = (0,1)\n",
    "            x, y = create_batch_xy(\n",
    "                        memory_length,\n",
    "                        transformed_np_arr,\n",
    "                        overlap=True,\n",
    "                        y_updown=False,\n",
    "                        diff_data=True,\n",
    "                        output_scale=OUTPUT_SCALE)\n",
    "        else:\n",
    "            x, x_ts, y = create_transformer_onehot_xy(\n",
    "                                memory_length,\n",
    "                                transformed_np_arr,\n",
    "                                DATAFRAME.to_numpy(),\n",
    "                                DATAFRAME.index.to_numpy(),\n",
    "                                0.002)\n",
    "\n",
    "        y_ts = timestamps_to_marks(\n",
    "            DATAFRAME.index.to_numpy()[memory_length:], 1)\n",
    "        split_idx = int(x.shape[0] * (1 - TEST_FRAC))\n",
    "\n",
    "        x_train = x[:split_idx,:,:]\n",
    "        x_ts_train = x_ts[:split_idx,:,:]\n",
    "        y_train = y[:split_idx,:,:]\n",
    "        y_ts_train = y_ts[:split_idx,:,:]\n",
    "\n",
    "        x_test = x[split_idx:,:,:]\n",
    "        x_ts_test = x_ts[split_idx:,:,:]\n",
    "        y_test = y[split_idx:,:,:]\n",
    "        y_ts_test = y_ts[split_idx:,:,:]\n",
    "\n",
    "        if CLASS_FIRST:\n",
    "            y_train = tf.transpose(y_train, (0,2,1))\n",
    "            y_test = tf.transpose(y_test, (0,2,1))\n",
    "\n",
    "        for head_size in [32]:\n",
    "            for num_heads in [16]:\n",
    "                for ff_dim in [32]:\n",
    "                    for num_transformer_blocks in [4]:\n",
    "                        for random_seed in [0]:\n",
    "                            if counter < RESUME:\n",
    "                                counter = counter + 1\n",
    "                                continue\n",
    "\n",
    "                            keras.utils.set_random_seed(random_seed)\n",
    "                            \n",
    "                            serial = \"transformer_model_\" \\\n",
    "                                + datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "                            model_dict = {\n",
    "                                \"serial\": serial,\n",
    "                                \"d_model\": head_size,\n",
    "                                \"dropout_rate\": 0.01,\n",
    "                                \"d_ff\": ff_dim,\n",
    "                                \"d_out\": 3,\n",
    "                                \"N\": num_transformer_blocks,\n",
    "                                \"M\": num_transformer_blocks,\n",
    "                                \"k\": 2,\n",
    "                                \"h\": num_heads,\n",
    "                                \"O\": 1,\n",
    "                                \"tau\": 10,\n",
    "                                \"seed\": random_seed\n",
    "                            }\n",
    "                            model = Autoformer(model_dict)\n",
    "\n",
    "                            model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                                        loss=keras.losses.CategoricalCrossentropy(),\n",
    "                                        metrics=[keras.metrics.Accuracy()])\n",
    "\n",
    "                            model.fit(x =\n",
    "                                    (x_train,x_ts_train,y_ts_train),\n",
    "                                    y = y_train,\n",
    "                                    batch_size=32,\n",
    "                                    epochs=50,\n",
    "                                    validation_split=0.25,\n",
    "                                    callbacks=[\n",
    "                                        keras.callbacks.EarlyStopping(\n",
    "                                            patience=5,\n",
    "                                            restore_best_weights=True\n",
    "                                        )\n",
    "                                    ])\n",
    "                            \n",
    "                            model.save(\"./models/\"+serial+\".keras\")\n",
    "\n",
    "                            model_dict.update(model.evaluate(\n",
    "                                        x =(x_test,\n",
    "                                            x_ts_test,\n",
    "                                            y_ts_test),\n",
    "                                        y = y_test,\n",
    "                                        batch_size=32,\n",
    "                                        workers=4,\n",
    "                                        use_multiprocessing=True,\n",
    "                                        return_dict=True\n",
    "                                    )\n",
    "                                )\n",
    "                            \n",
    "                            if os.path.exists(\"./transformer_results.json\"):\n",
    "                                with open(\"./transformer_results.json\", \"r\",\n",
    "                                        encoding=ENCODING) as json_file:\n",
    "                                    model_list = json.load(json_file)\n",
    "                                model_list.append(model_dict)\n",
    "                            else:\n",
    "                                model_list = [model_dict]\n",
    "\n",
    "                            with open(\"./transformer_results.json\", \"w\",\n",
    "                                    encoding=ENCODING) as json_file:\n",
    "                                json.dump(model_list, json_file)\n",
    "\n",
    "                            keras.backend.clear_session()\n",
    "\n",
    "    if os.path.exists(\"./transformer_results.json\"):\n",
    "        with open(\"./transformer_results.json\", \"r\",\n",
    "                  encoding=ENCODING) as json_file:\n",
    "            model_list = json.load(json_file)\n",
    "    else:\n",
    "        model_list = [{}]\n",
    "    \n",
    "    model_df = pd.DataFrame.from_dict(model_list)\n",
    "    model_df.to_excel(\"./transformer_results.xlsx\")\n",
    "else:\n",
    "    model = keras.models.load_model(\"./models/\" + MODEL_SERIAL + \".keras\")\n",
    "    memory_length = model.input_shape[0][1]-1\n",
    "\n",
    "    if PREDICT_PRICES:\n",
    "        OUTPUT_SCALE = (0,1)\n",
    "        x, y = create_batch_xy(\n",
    "                    memory_length,\n",
    "                    transformed_np_arr,\n",
    "                    overlap=True,\n",
    "                    y_updown=False,\n",
    "                    diff_data=True,\n",
    "                    output_scale=OUTPUT_SCALE)\n",
    "    else:\n",
    "        x, x_ts, y = create_transformer_onehot_xy(\n",
    "                            memory_length,\n",
    "                            transformed_np_arr,\n",
    "                            DATAFRAME.to_numpy(),\n",
    "                            DATAFRAME.index.to_numpy(),\n",
    "                            0.002)\n",
    "\n",
    "    split_idx = int(x.shape[0] * (1 - TEST_FRAC))\n",
    "\n",
    "    x_train = x[:split_idx,:,:]\n",
    "    x_ts_train = x_ts[:split_idx,:,:]\n",
    "    y_train = y[:split_idx,:,:]\n",
    "\n",
    "    x_test = x[split_idx:,:,:]\n",
    "    x_ts_test = x_ts[split_idx:,:,:]\n",
    "    y_test = y[split_idx:,:,:]\n",
    "\n",
    "    if CLASS_FIRST:\n",
    "        y_train = tf.transpose(y_train, (0,2,1))\n",
    "        y_test = tf.transpose(y_test, (0,2,1))\n",
    "plot_model(\n",
    "        model,\n",
    "        to_file=\"./figures/transformer_model_plot.png\",\n",
    "        show_shapes=True,\n",
    "        show_layer_names = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model([x_test, x_ts_test, x_test, x_ts_test], training=False)\n",
    "print(\"Y_pred: \\n\", y_pred)\n",
    "print(\"Y_test: \\n\", y_test)\n",
    "\n",
    "for stock_idx in range(y_pred.shape[2] if CLASS_FIRST else y_pred.shape[1]):\n",
    "    if CLASS_FIRST:\n",
    "        direction_preds = y_pred[:,:,stock_idx]\n",
    "        direction_true = y_test[:,:,stock_idx]\n",
    "    else:\n",
    "        direction_preds = y_pred[:,stock_idx,:]\n",
    "        direction_true = y_test[:,stock_idx,:]\n",
    "\n",
    "    direction_preds = np.argmax(direction_preds, axis=1)\n",
    "    direction_true = np.argmax(direction_true, axis=1)\n",
    "\n",
    "    accuracy = \\\n",
    "        np.sum(direction_preds == direction_true) / direction_preds.shape[0]\n",
    "    print(f\"\"\"\n",
    "            Up/Down/Flat accuracy for stock {IND_CONVERSION[stock_idx]}:\n",
    "            {accuracy}\n",
    "            \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profitability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate profit by optimal strategy (theoretical) vs using model to predict\n",
    "test_sz = int(DATAFRAME.shape[0] * TEST_FRAC)\n",
    "df_test = DATAFRAME.iloc[-test_sz:,:].copy()\n",
    "if HAS_TIMEDELTA:\n",
    "    df_test.drop(\"Time Delta\", axis=1, inplace=True)\n",
    "np_arr_test = df_test.to_numpy()\n",
    "print(\"np_arr_test data shape: \", np_arr_test.shape)\n",
    "\n",
    "optimal_trading_mask = calculate_optimal_invest_strategy(np_arr_test)\n",
    "print(f\"Optimal mask 3rd stock: \\n {optimal_trading_mask[:,2]}\")\n",
    "profit_optimal = calculate_profit_on_invest_strategy(np_arr_test,\n",
    "                                                     optimal_trading_mask)\n",
    "print(f\"Optimal strategy matrix shape: {optimal_trading_mask.shape}\")\n",
    "print(f\"Profit by optimal strategy on test data: {profit_optimal}\")\n",
    "\n",
    "# To calculate the mask for the model, we need to give the data in the same format as it was trained in\n",
    "transformed_df_test = transformed_df.iloc[-test_sz:,:]\n",
    "transformed_np_arr_test = transformed_df_test.to_numpy()\n",
    "print(\"transformed_np_arr_test data shape: \", transformed_np_arr_test.shape)\n",
    "print(transformed_np_arr_test[0:2,:])\n",
    "prediction_trading_mask = \\\n",
    "    strategy_mask_from_direction_model(transformed_np_arr_test,\n",
    "                                       memory_length, model,\n",
    "                                       True, df.to_numpy()[-test_sz:,:],\n",
    "                                       df.index.to_numpy()[-test_sz:]\n",
    "                                       )\n",
    "\n",
    "if HAS_TIMEDELTA:\n",
    "    prediction_trading_mask = prediction_trading_mask[:,1:]\n",
    "\n",
    "print(f\"Prediction mask 3rd stock: \\n {prediction_trading_mask[:,2]}\")\n",
    "if HAS_TIMEDELTA:\n",
    "    profit_pred_model = \\\n",
    "        calculate_profit_on_invest_strategy(np_arr_test[:,1:],\n",
    "                                            prediction_trading_mask)\n",
    "else:\n",
    "    profit_pred_model = \\\n",
    "        calculate_profit_on_invest_strategy(np_arr_test,\n",
    "                                            prediction_trading_mask)\n",
    "\n",
    "print(f\"Prediction strategy matrix shape: {prediction_trading_mask.shape}\")\n",
    "print(\n",
    "    f\"Profit by predicting the next hour using the model: {profit_pred_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_indices = np.random.choice(np.arange(len(IND_CONVERSION)), 6,\n",
    "                                 replace=False)\n",
    "part_mask = prediction_trading_mask[:,stock_indices]\n",
    "\n",
    "if HAS_TIMEDELTA:\n",
    "    part_price = np_arr_test[:,1:][:,stock_indices]\n",
    "else:\n",
    "    part_price = np_arr_test[:,stock_indices]\n",
    "\n",
    "ind_conversion = {si : IND_CONVERSION[i] for si, i in enumerate(stock_indices)}\n",
    "plt.figure(figsize=(16,9))\n",
    "plot_mask_and_data(part_mask, part_price, ind_conversion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
